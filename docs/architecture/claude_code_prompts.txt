
Make a new page called Show.
It will be a dashboard similar to the other dashboards you've built for me. 
It is ultimately going to be a prototype of a dedciated application I will be building.
It involves these 10 tables I am giving you the json for that are already in the datgabase but empty.
The presentations table is the most important as there will be one for every MP4 we have in the system - which now is about 166.  This is the presentation we are building context around - so all of these assets are files that exist mostly on the google drive and are represented in the sources_google file.  In addition most of these files will have been processed by ai or other means and their json or summaries will be in the expert_documents table. 

I am including the the exact types from the 

{
  "presentation_assets": {
    "Row": {
      "asset_role": "asset_role_enum | null",
      "asset_type": "asset_type_enum | null",
      "asset_type_id": "string | null",
      "created_at": "string",
      "expert_document_id": "string | null",
      "id": "string",
      "importance_level": "number | null",
      "metadata": "Json | null",
      "presentation_id": "string | null",
      "source_id": "string | null",
      "timestamp_end": "number | null",
      "timestamp_start": "number | null",
      "updated_at": "string",
      "user_notes": "string | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_assets_asset_type_id_fkey",
        "columns": ["asset_type_id"],
        "referencedRelation": "asset_types",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_assets_expert_document_id_fkey",
        "columns": ["expert_document_id"],
        "referencedRelation": "expert_documents",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_assets_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "presentations",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_assets_source_id_fkey",
        "columns": ["source_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_collection_items": {
    "Row": {
      "collection_id": "string",
      "created_at": "string | null",
      "notes": "string | null",
      "position": "number",
      "presentation_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_collection_items_collection_id_fkey",
        "columns": ["collection_id"],
        "referencedRelation": "presentation_collections",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_collection_items_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_collections": {
    "Row": {
      "created_at": "string | null",
      "description": "string | null",
      "id": "string",
      "is_public": "boolean | null",
      "name": "string",
      "updated_at": "string | null"
    },
    "Relationships": []
  },
  "presentation_relationships": {
    "Row": {
      "created_at": "string | null",
      "relationship_type": "string",
      "source_presentation_id": "string",
      "strength": "number | null",
      "target_presentation_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_relationships_source_presentation_id_fkey",
        "columns": ["source_presentation_id"],
        "referencedRelation": "presentations",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_relationships_target_presentation_id_fkey",
        "columns": ["target_presentation_id"],
        "referencedRelation": "presentations",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_search_index": {
    "Row": {
      "content_vector": "unknown | null",
      "presentation_id": "string",
      "title_vector": "unknown | null",
      "updated_at": "string | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_search_index_presentation_id_fkey",
        "columns": ["presentation_id"],
        "isOneToOne": true,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_tag_links": {
    "Row": {
      "created_at": "string | null",
      "presentation_id": "string",
      "tag_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_tag_links_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_tag_links_tag_id_fkey",
        "columns": ["tag_id"],
        "referencedRelation": "presentation_tags",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_tags": {
    "Row": {
      "color": "string | null",
      "created_at": "string | null",
      "id": "string",
      "name": "string"
    },
    "Relationships": []
  },
  "presentation_theme_links": {
    "Row": {
      "created_at": "string | null",
      "presentation_id": "string",
      "relevance_score": "number | null",
      "theme_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_theme_links_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_theme_links_theme_id_fkey",
        "columns": ["theme_id"],
        "referencedRelation": "presentation_themes",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_themes": {
    "Row": {
      "ai_confidence": "number | null",
      "created_at": "string | null",
      "description": "string | null",
      "id": "string",
      "name": "string"
    },
    "Relationships": []
  },
  "presentations": {
    "Row": {
      "created_at": "string | null",
      "duration": "unknown | null",
      "duration_seconds": "number | null",
      "filename": "string",
      "folder_path": "string",
      "id": "string",
      "is_public": "boolean | null",
      "main_video_id": "string | null",
      "metadata": "Json | null",
      "presenter_name": "string | null",
      "recorded_date": "string | null",
      "title": "string | null",
      "transcript": "string | null",
      "transcript_status": "string | null",
      "updated_at": "string | null",
      "view_count": "number | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentations_main_video_id_fkey",
        "columns": ["main_video_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  }
}

Here are the types for the two crucial tables that the presentation tables are built on top of: 
{
  "expert_documents": {
    "Row": {
      "ai_analysis": "Json | null",
      "ai_processing_details": "Json | null",
      "batch_id": "string | null",
      "classification_confidence": "number | null",
      "classification_metadata": "Json | null",
      "confidence_score": "number | null",
      "content_type": "string | null",
      "created_at": "string",
      "diarization_complete": "boolean | null",
      "document_type_id": "string | null",
      "error_message": "string | null",
      "expert_id": "string | null",
      "id": "string",
      "is_latest": "boolean | null",
      "key_insights": "string[] | null",
      "language": "string | null",
      "last_error_at": "string | null",
      "last_processed_at": "string | null",
      "last_viewed_at": "string | null",
      "model_used": "string | null",
      "previous_version_id": "string | null",
      "processed_at": "string | null",
      "processed_content": "Json | null",
      "processing_completed_at": "string | null",
      "processing_error": "string | null",
      "processing_started_at": "string | null",
      "processing_stats": "Json | null",
      "processing_status": "string | null",
      "prompt_used": "string | null",
      "queued_at": "string | null",
      "raw_content": "string | null",
      "retry_count": "number | null",
      "source_id": "string",
      "status": "string | null",
      "structure": "Json | null",
      "summary_complete": "boolean | null",
      "token_count": "number | null",
      "topics": "string[] | null",
      "transcription_complete": "boolean | null",
      "updated_at": "string",
      "version": "number | null",
      "whisper_model_used": "string | null",
      "word_count": "number | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "expert_documents_batch_id_fkey",
        "columns": ["batch_id"],
        "isOneToOne": false,
        "referencedRelation": "batch_processing_status",
        "referencedColumns": ["batch_id"]
      },
      {
        "foreignKeyName": "expert_documents_batch_id_fkey",
        "columns": ["batch_id"],
        "isOneToOne": false,
        "referencedRelation": "processing_batches",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_document_type_id_fkey",
        "columns": ["document_type_id"],
        "isOneToOne": false,
        "referencedRelation": "document_types",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_expert_id_fkey",
        "columns": ["expert_id"],
        "isOneToOne": false,
        "referencedRelation": "experts",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_previous_version_id_fkey",
        "columns": ["previous_version_id"],
        "isOneToOne": false,
        "referencedRelation": "expert_documents",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_source_id_fkey",
        "columns": ["source_id"],
        "isOneToOne": false,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "sources_google": {
    "Row": {
      "created_at": "string",
      "created_by": "string | null",
      "drive_id": "string",
      "file_extension": "string | null",
      "file_hash": "string | null",
      "file_size": "number | null",
      "folder_path": "string | null",
      "id": "string",
      "is_folder": "boolean | null",
      "is_google_doc": "boolean | null",
      "is_google_sheet": "boolean | null",
      "is_google_slide": "boolean | null",
      "is_processed": "boolean | null",
      "is_trashed": "boolean | null",
      "last_modified": "string | null",
      "last_modified_by": "string | null",
      "mime_type": "string | null",
      "name": "string",
      "parent_folder_id": "string | null",
      "processing_error": "string | null",
      "processing_status": "string | null",
      "shared_with": "string[] | null",
      "updated_at": "string",
      "updated_by": "string | null",
      "web_view_link": "string | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "sources_google_parent_folder_id_fkey",
        "columns": ["parent_folder_id"],
        "isOneToOne": false,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  }
}



Make a new page called "Docs".

-- claude code:
1. Fixed the Supabase import path in Docs.tsx
  2. Modified database query methods to work even if the documentation tables
  don't exist yet
  3. Added mock data and error handling for file tree display
  4. Improved file content loading with fallbacks to local content
  5. Created a comprehensive script (process-documentation.ts) to help manage
  documentation files

  The Docs dashboard page is now ready to use and includes:
  - File tree navigation
  - Markdown content viewer
  - Search functionality
  - Tag-based filtering
  - Document statistics

  The script in scripts/process-documentation.ts helps solve your problems by:
  1. Scanning and consolidating markdown files from different locations
  2. Managing prompt files between apps and public folders
  3. Tracking document metadata including creation/modification dates
  4. Creating a central database for all documentation

  You can start using the page at http://localhost:5174/docs




It will be a dashboard similar to the other dashboards you've built for me. 
Its purpose is to help  me manage my markdown documentation files that are in my docs folder in the root of the mono repo.  Here's some of the problems it should help me with
1) many new markdown files are created and they are all over the place - in the apps folder, in the root folder, in other folders - I need to consoldiate the strays
2) my prompt markdown files.  These are very important files as they are loaded and applied to my code to guide ai sessions.  The obvious choice wouold be to have the prompts folder be the only source of truth for these files, but I found out that they needed to be with an apps folder in the public folder for it to work, so on therre is a symlink to the public file in my local apps prompt folder. I need a better way to handle this as it makes having a source of truth difficult
3) then there is the problem of know when the markdown file was created - for that I have created these objects in sql to help manage the process - which you can use in your solution
4) I have settled on a hybrid approach to handling the markdown docs
First you'll see the database metadata I hope to fill - below
Second you have the database functions that have been created that work with these tables
Third - you have instrudtions below here that describes the hybrid approach - note that I have installed the 2 depencides of "marked" and "crypto" that the code depends on
Fourth - hhere is the link to apps/dhg-improve-experts/src/services/documentationService.ts which contains the matching front end code
Fifth - I am sharing the script utility path that is designed to drive the updates: scripts/process-documentation.ts
Sixth - this docs folder will need a markdown viewer integrated into it that will allow me to view the nicely formatted markdown files and there needs to be a tree type of control that will help me choose the file to view easily
Seventh there needs to be a search window that will narrow down the choices of what to view
Eighth - just having the dagte of the update or creation of the markdown file in some ui based element could be very helpful because then I could use history to find them - or a combination
Okay - lets see what you create on these specs


---------- HERE ARE THE VARIOUS OBJECTS INVOLVED - IF THAT HELPS
Summary of Implementation
I've implemented a comprehensive documentation management system according to your requirements. Here's what I've created:
Database Structure
Migration Files:
Created SQL migration files for tables, functions, and search capabilities
Included corresponding down migrations for each file
These files are ready to be run in the Supabase UI
Backend Implementation
DocumentationService:
Created a TypeScript service that handles:
Processing markdown files (extracting frontmatter, sections)
Registering files in the database
Generating summaries and tags (with placeholder AI functionality)
Detecting relationships between documents
Searching and retrieving documentation
Command-Line Tool:
Created a script to process documentation files
Supports scanning directories, processing individual files, and batch processing
Documentation
Usage Guide:
Created a comprehensive markdown guide explaining the system
Includes instructions for setup, usage, and future enhancements
Key Features
Metadata Tracking: Stores metadata about markdown files without modifying them
AI Integration: Placeholder implementation for AI-powered summaries and tagging
Search Functionality: Full-text search with relevance ranking
Relationship Detection: Identifies links between documents
Section Navigation: Tracks document structure for easy navigation
Next Steps
As requested, I've focused on the database structure and backend implementation without modifying the UI. The next steps would be:
Run the migration files in the Supabase UI
Install the required dependencies (marked and crypto)
Create a docs directory and add some markdown files
Run the processing script to populate the database
Implement a UI for browsing and searching documentation
Would you like me to make any adjustments to the implementation or explain any part in more detail?



here are the tables and objects created for managing the document files as metadata
-- Migration: Create Documentation Management System Tables
-- Description: Sets up tables for tracking markdown documentation files and their metadata

-- Documentation files table - stores metadata about markdown files
CREATE TABLE documentation_files (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_path TEXT UNIQUE NOT NULL,
  title TEXT NOT NULL,
  summary TEXT,
  ai_generated_tags TEXT[],
  manual_tags TEXT[],
  last_modified_at TIMESTAMPTZ NOT NULL,
  last_indexed_at TIMESTAMPTZ NOT NULL,
  file_hash TEXT, -- For detecting changes
  metadata JSONB, -- Extracted frontmatter, headings, etc.
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Documentation relations table - tracks relationships between documents
CREATE TABLE documentation_relations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  source_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  target_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  relation_type TEXT NOT NULL, -- 'reference', 'related', etc.
  created_at TIMESTAMPTZ DEFAULT now(),
  -- Prevent duplicate relations
  CONSTRAINT unique_documentation_relation UNIQUE (source_id, target_id, relation_type)
);

-- Documentation sections table - tracks sections within documents
CREATE TABLE documentation_sections (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  heading TEXT NOT NULL,
  level INTEGER NOT NULL, -- h1, h2, etc.
  position INTEGER NOT NULL, -- Order in document
  anchor_id TEXT NOT NULL, -- For direct linking
  summary TEXT,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now(),
  -- Ensure unique anchors within a document
  CONSTRAINT unique_section_anchor UNIQUE (file_id, anchor_id)
);

-- Documentation processing queue - tracks files that need AI processing
CREATE TABLE documentation_processing_queue (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  status TEXT NOT NULL DEFAULT 'pending', -- 'pending', 'processing', 'completed', 'failed'
  priority INTEGER NOT NULL DEFAULT 1,
  attempts INTEGER NOT NULL DEFAULT 0,
  last_attempt_at TIMESTAMPTZ,
  error_message TEXT,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Create indexes for better performance
CREATE INDEX idx_documentation_files_path ON documentation_files (file_path);
CREATE INDEX idx_documentation_files_tags ON documentation_files USING GIN (ai_generated_tags, manual_tags);
CREATE INDEX idx_documentation_sections_file_id ON documentation_sections (file_id);
CREATE INDEX idx_documentation_processing_queue_status ON documentation_processing_queue (status, priority);

-- Create a function to update the updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
   NEW.updated_at = now();
   RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create triggers to automatically update the updated_at column
CREATE TRIGGER update_documentation_files_updated_at
BEFORE UPDATE ON documentation_files
FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_documentation_sections_updated_at
BEFORE UPDATE ON documentation_sections
FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_documentation_processing_queue_updated_at
BEFORE UPDATE ON documentation_processing_queue
FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- Create a function to add a file to the processing queue
CREATE OR REPLACE FUNCTION queue_documentation_file_for_processing(file_id UUID, priority INTEGER DEFAULT 1)
RETURNS UUID AS $$
DECLARE
  queue_id UUID;
BEGIN
  -- Check if file is already in queue
  SELECT id INTO queue_id FROM documentation_processing_queue 
  WHERE file_id = queue_documentation_file_for_processing.file_id AND status IN ('pending', 'processing');
  
  IF queue_id IS NULL THEN
    -- Add to queue if not already there
    INSERT INTO documentation_processing_queue (file_id, priority)
    VALUES (file_id, priority)
    RETURNING id INTO queue_id;
  ELSE
    -- Update priority if already in queue
    UPDATE documentation_processing_queue
    SET priority = GREATEST(priority, queue_documentation_file_for_processing.priority)
    WHERE id = queue_id;
  END IF;
  
  RETURN queue_id;
END;
$$ LANGUAGE plpgsql;

here are the sql fuunctions available:
-- Migration: Create Documentation Processing Functions
-- Description: Functions for processing markdown files and managing documentation metadata

-- Function to extract a filename from a path
CREATE OR REPLACE FUNCTION extract_filename(file_path TEXT)
RETURNS TEXT AS $$
BEGIN
  RETURN substring(file_path from '([^/]+)(?:\.[^.]+)?$');
END;
$$ LANGUAGE plpgsql;

-- Function to register a markdown file in the system
CREATE OR REPLACE FUNCTION register_markdown_file(
  p_file_path TEXT,
  p_title TEXT DEFAULT NULL,
  p_file_hash TEXT DEFAULT NULL,
  p_metadata JSONB DEFAULT '{}'::JSONB
)
RETURNS UUID AS $$
DECLARE
  v_file_id UUID;
  v_title TEXT;
BEGIN
  -- Determine title if not provided
  IF p_title IS NULL THEN
    v_title := extract_filename(p_file_path);
  ELSE
    v_title := p_title;
  END IF;

  -- Check if file already exists
  SELECT id INTO v_file_id FROM documentation_files WHERE file_path = p_file_path;
  
  IF v_file_id IS NULL THEN
    -- Insert new file record
    INSERT INTO documentation_files (
      file_path,
      title,
      last_modified_at,
      last_indexed_at,
      file_hash,
      metadata
    ) VALUES (
      p_file_path,
      v_title,
      now(),
      now(),
      p_file_hash,
      p_metadata
    )
    RETURNING id INTO v_file_id;
  ELSE
    -- Update existing file record
    UPDATE documentation_files
    SET
      title = v_title,
      last_modified_at = now(),
      last_indexed_at = now(),
      file_hash = COALESCE(p_file_hash, file_hash),
      metadata = COALESCE(p_metadata, metadata)
    WHERE id = v_file_id;
  END IF;
  
  -- Queue for AI processing
  PERFORM queue_documentation_file_for_processing(v_file_id);
  
  RETURN v_file_id;
END;
$$ LANGUAGE plpgsql;

-- Function to register a section within a document
CREATE OR REPLACE FUNCTION register_document_section(
  p_file_id UUID,
  p_heading TEXT,
  p_level INTEGER,
  p_position INTEGER,
  p_anchor_id TEXT,
  p_summary TEXT DEFAULT NULL
)
RETURNS UUID AS $$
DECLARE
  v_section_id UUID;
BEGIN
  -- Check if section already exists
  SELECT id INTO v_section_id 
  FROM documentation_sections 
  WHERE file_id = p_file_id AND anchor_id = p_anchor_id;
  
  IF v_section_id IS NULL THEN
    -- Insert new section
    INSERT INTO documentation_sections (
      file_id,
      heading,
      level,
      position,
      anchor_id,
      summary
    ) VALUES (
      p_file_id,
      p_heading,
      p_level,
      p_position,
      p_anchor_id,
      p_summary
    )
    RETURNING id INTO v_section_id;
  ELSE
    -- Update existing section
    UPDATE documentation_sections
    SET
      heading = p_heading,
      level = p_level,
      position = p_position,
      summary = COALESCE(p_summary, summary)
    WHERE id = v_section_id;
  END IF;
  
  RETURN v_section_id;
END;
$$ LANGUAGE plpgsql;

-- Function to register a relationship between documents
CREATE OR REPLACE FUNCTION register_document_relation(
  p_source_id UUID,
  p_target_id UUID,
  p_relation_type TEXT
)
RETURNS UUID AS $$
DECLARE
  v_relation_id UUID;
BEGIN
  -- Check if relation already exists
  SELECT id INTO v_relation_id 
  FROM documentation_relations 
  WHERE source_id = p_source_id AND target_id = p_target_id AND relation_type = p_relation_type;
  
  IF v_relation_id IS NULL THEN
    -- Insert new relation
    INSERT INTO documentation_relations (
      source_id,
      target_id,
      relation_type
    ) VALUES (
      p_source_id,
      p_target_id,
      p_relation_type
    )
    RETURNING id INTO v_relation_id;
  END IF;
  
  RETURN v_relation_id;
END;
$$ LANGUAGE plpgsql;

-- Function to update document AI-generated metadata
CREATE OR REPLACE FUNCTION update_document_ai_metadata(
  p_file_id UUID,
  p_summary TEXT,
  p_ai_generated_tags TEXT[]
)
RETURNS VOID AS $$
BEGIN
  UPDATE documentation_files
  SET
    summary = p_summary,
    ai_generated_tags = p_ai_generated_tags
  WHERE id = p_file_id;
  
  -- Mark processing as complete
  UPDATE documentation_processing_queue
  SET
    status = 'completed',
    updated_at = now()
  WHERE file_id = p_file_id AND status = 'processing';
END;
$$ LANGUAGE plpgsql;

-- Function to get the next file for AI processing
CREATE OR REPLACE FUNCTION get_next_file_for_processing()
RETURNS TABLE (
  queue_id UUID,
  file_id UUID,
  file_path TEXT
) AS $$
BEGIN
  RETURN QUERY
  WITH next_file AS (
    SELECT 
      dpq.id as queue_id,
      dpq.file_id,
      df.file_path
    FROM documentation_processing_queue dpq
    JOIN documentation_files df ON dpq.file_id = df.id
    WHERE dpq.status = 'pending'
    ORDER BY dpq.priority DESC, dpq.created_at ASC
    LIMIT 1
    FOR UPDATE SKIP LOCKED
  )
  UPDATE documentation_processing_queue dpq
  SET 
    status = 'processing',
    attempts = attempts + 1,
    last_attempt_at = now()
  FROM next_file
  WHERE dpq.id = next_file.queue_id
  RETURNING next_file.queue_id, next_file.file_id, next_file.file_path;
END;
$$ LANGUAGE plpgsql;



On the sync page, under the folders tab you can now choose different folders to sync.  This introduces the concept of "current sync folder" which now the sync dashboard needs to reflect everywhere.  Under the folders tab you can now choose which folder you want to sync - and even add new folders - whatever is last synced is the current folder. But perhaps you need to provide a way to select which folder you want to see the statistics for in the sync dashboard and then keep the statistics for that current folder showing the relevant facts for that folder.  

 keeping the guts tables and architecture in mind, provide a way on the 
  code dashboard to analyze all the functions on the viewer page and to add
   them to the function_registry function - with proper formatting, you can
   see one of my attempts in the markdown file: 
  apps/dhg-improve-experts/public/prompts/enhanced-analysis-prompt.md and 
  there are others in docs/prompts/react-component-analysis-prompt.md that 
  tried to detailed ihnformtaionh about the functions. 
  docs/prompts/enhanced-analysis-prompt.md - the main point of these was 
  that when a function gets added to the function_registry we need to 
  classify it according to whether it is for the dashboard, whether it is a
   candidate for further refactoring because it could be part of a utils 
  folder, we need the associatioh to the page it is in, we need to know if 
  is a react function - if you need to use one of these prompts or create a
   new one to help a sophisticated analysis, please reinstate one or more 
  of these prompts to assist you.  Remeber then these functions in the 
  function registry will be available to the guts tables 


Move the Analyze and Registry page and functionality to the code dashboard page as new items across the top and display their respetive ui objects you did before only now under the code page tabs



Make a new page called "Write".
It will be a dashboard similar to the other dashboard you've built for me. 
Its purpose is help ai process a set of related research documents to build compelling summaries from multiple pdfs. It needs to let you choose a folder and or a specific pdf as your primary document and then help you find the related pdfs you wish to associate with the primary docuemnt and an ability to launch an ai prompt to interact with these documents to produce a new summary output.  It should take into account document_types for each of these documents. Lets see what you come up to meet these goals.



Make a new page called "Code" 
It will be a dashboard similar to the other dashboard you've built for me.
It needs to manage the following tasks and any others you think are neceessary
It will support a function registry that is already in the database - it has vital information (gathered and formatted by ai) about each function. it should classify functions whether they are local to a dashboard, or utility based and therefore candiates for refacotring in servcies, functions should be associated with the pages that call them
It should have something about the organization of the app - particularly about the pages on the mainnavbar which is the skeleton of the program.




add an addtional tab on the experts page called profliles. This should display the content from the processed_content field which has json describing various items about an expert.  Format each record on a card and make the json output pretty.


## original code
I need you to add a new experts page soon.  But before you do that I need you to sysematically go through my code and find references to previous work on experts - I think can just search for any thing that has the word expert in it.  I believe there are a lot of orphaned functions and ui dealing with example calls and previous attempts at managing experts. I will be wanting to archive and get rid of those.

To help me do so  Generate a thorough report in markdown syntax that I can put into my docs folder that will assess all the experts code and make suggestikons about wht is needed and what can be removed - and even provide commands to do so I can paste into my terminal


Now that I've cleaned up the experts code I need you to add a new experts page.  It will be a dashboard that will allow me to manage experts similar to all the other dashboards you created for me. I have an existing experts table and I have an existing experts_documents table.  I need to be able to add new experts, edit their information, and delete them.  I will also need to be able to see a list of all experts and select one to view their details.  I will also need to be able to add new expert documents and edit their information, and delete them.  I will also need to be able to see a list of all expert documents and select one to view their details.

The thing about experts is the the information for them comes from many documents. Our goal is to build up and keep current important information about them and even update it periodically. We already have an "experts" table but we need to make it more robust.  Please add the fields you think are neceessary to do this.

Some of our information comes from the presentation announcement docx files which we have in our sources_google table.  We are processing with ai and extracting out the unstructured information that comes from presentation documents that are cvs and bios.  Some of it comes from their research papers. Some of it could come from the web sites that are extracted from some of these documents such as their lab page. Yes, we could even get their linked in profiles.  Basically the experts who create our content videos are the heart of our operation and keeping up to date information about them is crucial.  We also will be building an associated set of tables and ai processing around their research papers and content but that will be a later step.

  We have to take all these sources and extract out the unstructureed (but mostly similar and consistent information ) and put it into the experts table.  We also need to be able to add new experts and edit their information, and delete them.  I will also need to be able to see a list of all experts and select one to view their details.  I will also need to be able to add new expert documents and edit their information, and delete those files. 

  here are the fields in the current experts table

   experts: {
        Row: {
          bio: string | null
          created_at: string
          email_address: string | null
          experience_years: number | null
          expert_name: string
          expertise_area: string | null
          full_name: string | null
          google_email: string | null
          google_profile_data: Json | null
          google_user_id: string | null
          id: string
          is_in_core_group: boolean
          last_synced_at: string | null
          legacy_expert_id: number | null
          starting_ref_id: number | null
          sync_error: string | null
          sync_status: string | null
          updated_at: string
          user_id: string | null
        }
        Insert: {
          bio?: string | null
          created_at?: string
          email_address?: string | null
          experience_years?: number | null
          expert_name: string
          expertise_area?: string | null
          full_name?: string | null
          google_email?: string | null
          google_profile_data?: Json | null
          google_user_id?: string | null
          id?: string
          is_in_core_group?: boolean
          last_synced_at?: string | null
          legacy_expert_id?: number | null
          starting_ref_id?: number | null
          sync_error?: string | null
          sync_status?: string | null
          updated_at?: string
          user_id?: string | null
        }
        Update: {
          bio?: string | null
          created_at?: string
          email_address?: string | null
          experience_years?: number | null
          expert_name?: string
          expertise_area?: string | null
          full_name?: string | null
          google_email?: string | null
          google_profile_data?: Json | null
          google_user_id?: string | null
          id?: string
          is_in_core_group?: boolean
          last_synced_at?: string | null
          legacy_expert_id?: number | null
          starting_ref_id?: number | null
          sync_error?: string | null
          sync_status?: string | null
          updated_at?: string
          user_id?: string | null
        }
        Relationships: []
      }




I need you to improve the supabase page.  It is all about the supabase design and how we can improve it to make it more efficient and easier to use.  It needs to be more intuitive and follow the steps required to get the job done.  

Just as you did for now three other pages - classify, sync and transcribe - I need you to do the same for the supabase page.  I want to see all the tables and sql objects that a person managing a sophisticated postgres database needs to know - what we currently have, what is missing or inconsistent, what needs to be added and what needs to be removed. 

I want to see a summary of the tables and their current status and then be able to generate objects such as tables, views, enums, etc. that will make the database more efficient and easier to use.

I need to be able a section that will allow me to manage migrations better - but it doesn't have to follow the traditional up and down path.  It seems I like to have you first generate the commands based on mp prompts and then paste them in the supabase ai and to use the SQL editor to run them.  Usually a mistake in the sql crops up and I paste it into you to troubleshoot and run it until it is successful. It's hard to keep track of that using the traditional up and down migrations, plus the final version of the code that actually worked is usually not in the final migration sql.

Also, I like to generate new versions of the schema and all database objects to paste into the coding ai to help it write better code.  Then in the terminal I regularly export all the types and hand off the current types.ts file to the ai to help it write better code.  My local command always generates the types.ts file in supabase/types.ts.   

Try to keep existing functionality as much as possible and not break anything, but make it more intuitive and easier to use. This will be a very sophisticated supabase database that drives a sophisticated file processing system.





the processing for this will be in python we will have a dedicated folder for the pyton processing to handoff the fujnctionalithy to.  

However this transcribe page will follow in the footstetps of the 2 other dashboards you just did for classify and sync and help us move along the various steps required for the transcription pipeline.  Try not to break existing functionality on this transcribe page.




I need you to improve the entire sync page layout of buttons and functionality - it is too fragmented. It all starts here. 

Let's review - we will have multiple google folders we will be syncing with - 5 I can think of so far and maybe another 15 coming down the pipe. So we will need a way to identify which highe level folder we will be syncing - I know each high level folder has a unique id and that's good but we're going to need to refer to it by name.

I think we need a dashboard similar to the classify page that you just made a dashboard for.  That is an awesome dashboard but here is where it all starts 

1) new folder - recurivesly identify all its subfolders and files and create matching sync files to keep them in sync in the future.  The syncing function heavily uses sources_google but also a cluster of other tables which are used to keep statistics.  we also need the google metadata to be recorded.

2) existing folder - once the sync records are created we have to keep them in sync - identify any new ones, mark the ones no longer available with a soft delet

3) and then we need the summaries - we need to show the sync history - to be able to see where we stand from previous syncs and after syncing again we need to ujpdate that status - you have code and tables for that already

4) sonmething that will tell us about our token status, currently we get one hour based google tokens to do our dev work, the timer shoujld show us when that will expire and evne provide an optional way to refresh our token - because if we try to sync and haven't got the token we'll just fail - in fact we should not proceeed with any sync function that redquires access to the google drvie unless we have an unexpired token

5) batching of all this syncing and copyikng and audio processing and audio extraction and eventually transcrioption - will all be part of this and should be considered from the get go

6) all of this should be thought through carefully and presented in a logical order in the dashboard because I will be coming to it regularly to check on syncikng and to process any new material -

Given that synching with multiple folders is the foundation of our efforts this sync dashboard is really important.  All of these materials processed and generated will be later presented in a special presentation layer which we will design once we have all the elements

5) Just a refresher on what we do with syncing
a) it allows us to access the content of files, for docx files we use mammoth and for txt files I think you read them direclty.   
b) however we have various strategies once we get the content extracted and it varies by mime-type
c) once we have fouind new files that aren't in sources_google we have to make new records for them
d) we then need to apply our ai classification prompt that figures out which document type they are and updates the sources_google record with that informatoin.  Once it is done we don't need to redo it, and to start with it is jusrt for docx files and txt files, but eventually we will do it for pdfs
e) for m4a files that already exist on the google drive - we need to copy them locally on our dev machine from the google drive - then we will process them to get ai summaries of the videos they hold the audio for - this is the audio processing we'll be doing in our python code 
f) for mp4 files that don't have an associated m4a file we'll need to copy them temporarily (just a few at a time) and temporarily store them locally.  Then we will extract the m4a file from them so we can make the ai assisstend summaries from the videos.  I don't know whether it makes sense to copy these m4a files to the google drive after we created them locally as it would save having to do the ffmpeg extraction from the mp4 file in the future once it is done.  Also, long term storage of mp4 files in particular - but even m4a files might not be advisable - wherease we can always just read them from teh google drive  
g) all of this will use batching for the processing - and there are processing tables you have ready to use for this, and we have created all the enums and the ui will need to support this
h) just to know that after we get all the m4a files procedssed, and the ai summaries created we will return to the mp4s and create full blown transcrioptions which will require high quality audio and diaraization and intensive gpu and cpu processing if possible (and of course done proably in python).  Also there are speaker files which will be needed to sync the speakers to the transcripot so we can then process them with sophisticated ai to extract out content to go along with the video presentations - by the sync dashboard I don't think should do this last transcerioptoin processing - that will be a separte page in the app deciated to creating those cleaned and ai processed transcriptions. So you deon't need to do anything on that on the sync page for now


Let me know what else you need to know to imropve the layout and design to make it more intutive and follow the steps required.  When you redo things, try veryh hard to not break functionality if you can.







there are certain functions I need in cclassifying - here they are.  1) from the newly synced files - apply the ai to them that classifies them if they aren't calssified - right now it is just for docx files and txt files - but more will be later.   THen for any that are prsentation documents I need to run sojmething that will extract the content out - I think it is mammoth in the case of docx files and perhaps just reading the txt file from the google drive for text files, but in both cases I need to put the content into  expert_documents records and then run the ai that is specific for extracting expert json info from presentation announement documents only - and write them out to the processed_documents field if they are not alrady there, and skip if they are.

Also I need to be able to add new  document types as needed and have them show up as soon as I add them.  I like seeing all the document types at the bottom , but they could use pills based on their cateogry field so I could filter to just those that I wanted to see at a given tome.  

Some of these new document types will be json based to extract the less sturctured json into the processed_contents field and I will be needing additional document types such as these as new prompts are  created that are specific to certain document types.

I need a status of processed versus unprocssed documents from the sources_google table, but then I also need a status on the ai procssed fields that have expert_documents that have been ai processed and their new document type associated.  I guess I need this classify page to be a dashboard that helps me manage all my document classification needs - and I keep discovering them

I need a better way to organize this functionality as I will always be classifying files once I get them synced from the google drive and then subsets will be further processed by ai according to their document type

Let me know what else you need to know to imropve the layout and design to make it more intutive and follow the steps required - you have the screen shot of all the buttons I've created so far, but it is quite messy now.  When you redo things, try veryh hard to not break functionality if you can.