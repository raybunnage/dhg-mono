cleanup markdown files:

I've created a comprehensive solution for organizing documentation in
  your monorepo. Here's what I've implemented:

  1. Documentation Organization Scripts:
    - consolidate-docs.sh: Moves markdown files to the appropriate
  locations
    - generate-docs-report.sh: Creates a report of all markdown files
    - add-frontmatter.sh: Adds YAML frontmatter to documentation files
    - tree-docs.sh: Shows tree structure of documentation files by app
    - docs-index.js: Generates a JSON index of all docs for your
  dashboard
    - run-all.sh: Master script to run all the above in sequence
  2. NPM Scripts in package.json:
  "docs:report": "./scripts/docs-organization/generate-docs-report.sh",
  "docs:tree": "./scripts/docs-organization/tree-docs.sh",
  "docs:organize": "./scripts/docs-organization/run-all.sh",
  "docs:consolidate": "./scripts/docs-organization/consolidate-docs.sh",
  "docs:frontmatter": "./scripts/docs-organization/add-frontmatter.sh"
  3. Documentation Standards:
  Created a new file at docs/docs-organization.md with best practices and
   guidelines.

  To use these tools:

  1. Run npm run docs:report to see the current state of documentation
  2. Run npm run docs:tree [app-name] to view the document tree for a
  specific app
  3. Run npm run docs:organize to reorganize all documentation according
  to standards

  The scripts preserve original files while generating standardized
  copies, so you can review changes before committing.


in the process of developoment all kinds of markdown files are crated all over tghe place (since this is a mono repo.  I am trying to make things as consistent as possible, and keep things in one docs folder which is in the root dir but except for the special case of prompts - which I am tryhing to sort out - which is a folder underneat the docs root folder, I want things to be simpole, and because I am working on one particular monolithic tooling app which is just one of the apps in myu mono repo I end up with markdown documnentation files and readme files created by ai - all over the place.  What is the best practice in a mono repo - I want to keep everhthing in one root docs folder because in the future many of my apps will share code and I don't want duoplication.  On the other hand there are legimate reasons to have a docs folder within a project for poroject specific information - or is it necessary - ofen then it adds to confusion.  Help straighten my thinkikng out on this.  Also know that I am building tooling that mostly favors the root docs folder - and a dedicated dashboard that will display these markdown document files from my local development machine in a viewer that will read and display the files.  This is a factor in my question as well.  Please advise so I can sort through this.  Settling on my approach will help make my repository less messy.  


HEre are your instructions:
Okay imoplement this but follow my cursorrules for the agent
1) do no harm to existing code or ui
2) do not make ui changes with out first asking me
3) if necessary give me any simple dataase imp;rovments that might help with this - but only do so if necessary 
4) create sceriopts to do these things for the markdown files and foldder - to improve the organization according to the principles you just laid out
5) leave any files that are under prompts folders alone - whether they are under the root docs folder or within the public folder or anywhere else in the apps folders - I am still trying to figure out how to deal with them
6) perfect a tree command that passes in the name of the app that it applies to - I think you will find one in the package.json file I am incluiding:  
7) create a report of what you find from scanning the tree command with regard to all the markdown files - I do want to have them in the local directory - but only following the riules you specified


Below is the plan that was created by ai to execute on my questions above.
Figure out the best way to implement this plan and go for it


Documentation Organization in a Monorepo
You're facing a common challenge in monorepos: balancing centralized documentation with project-specific docs. Let me help clarify some best practices and suggest an approach that aligns with your tooling goals.
Best Practices for Documentation in Monorepos
Recommended Structure
the
Principles to Follow
Single Source of Truth: Keep shared, high-level documentation in the root /docs folder.
README.md in Every Project: Each app/package should have a README.md that:
Briefly describes what the project does
Links to relevant documentation in the root /docs folder
Contains quick start instructions
Does NOT duplicate information from the root docs
Minimize Project-Specific Docs Folders: Only create a /docs folder within a project if:
The documentation is truly specific to that project's implementation
The documentation would not make sense in any other context
The documentation is extensive enough to warrant its own folder
Cross-Reference Instead of Duplicate: When a project needs to reference shared concepts, link to the root docs rather than duplicating information.
Specific Recommendations for Your Case
Since you're building tooling that favors the root docs folder and a dedicated dashboard viewer, I recommend:
Centralize Most Documentation: Keep as much as possible in the root /docs folder, organized by topic rather than by project.
Use Frontmatter for Organization: Add YAML frontmatter to your markdown files to indicate which projects they apply to:
Implementation
Create a Documentation Index: Maintain an index file that maps documentation to projects, which your dashboard can use to filter relevant docs.
Standardize README.md Files: Create a template for project README.md files that includes:
Project description
Quick start
Links to relevant documentation in the root docs folder
Development workflow
Documentation Migration Plan: If you already have docs scattered throughout the repo:
Create a script to identify all markdown files
Evaluate each for whether it should be moved to the root docs
Update references to maintain links
Implementation for Your Dashboard
For your dashboard that displays markdown files:
Configure to Prioritize Root Docs: Set up your dashboard to primarily look at the /docs folder.
Add Project Context: When displaying a document, show which projects it applies to.
Implement Search Across All Docs: Allow users to search across all documentation, regardless of location.
Provide Filtering: Let users filter docs by project, category, or other metadata from frontmatter.
This approach gives you the best of both worlds: centralized documentation for shared concepts and cross-cutting concerns, while still allowing for project-specific information where truly needed. It also aligns well with your goal of building tooling around the root docs folder.





on the code dashboard make a new tab called cleanup which should help with the following
1) run a tree command (that ignores config files and .env files and node files and others it shouldn't track), but shows all the files in the entire repo. But it shoujld primarily be for cleanup around a given app which somehow needs to be passed in as a   This tree command should then be passed to an analysis file that will help identify potential dups and files that are out of place




Make a new page called Cmds 
It will be a dashboard similar to the other dashboards you've built for me. 
belwo is a spec of what I have so far to work with - the database tables and the front end code and the scripts - use these to guide your solution. Also a markdown file explaining about the system. Use these within your dashboard solution plus anything else is useful in building a cmds dasbhoard to help manaage the complexity of the different commands that need to be called regulalry (one off commands and special cirumstance we will still call in the terminal - and our dashboard is not designed to replace the terminal - just to help manage the complexity of commands in development )

-- here are the sql tables an functions that got create in the database:

-- Create extension for UUID generation if not exists
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Create Command Categories Table
CREATE TABLE command_categories (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  name TEXT NOT NULL UNIQUE,
  description TEXT,
  color TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Insert some initial categories
INSERT INTO command_categories (name, description, color) VALUES
('git', 'Git version control commands', '#F05032'),
('pnpm', 'Package management commands', '#F9AD00'),
('build', 'Project build commands', '#2B7489'),
('deploy', 'Deployment related commands', '#3178C6'),
('database', 'Database operations', '#336791'),
('system', 'System administration commands', '#4EAA25'),
('other', 'Miscellaneous commands', '#808080');

-- Create Command History Table
CREATE TABLE command_history (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  command_text TEXT NOT NULL,
  sanitized_command TEXT NOT NULL, -- Version with potential secrets removed
  category_id UUID REFERENCES command_categories(id),
  executed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  duration_ms INTEGER,
  exit_code INTEGER,
  success BOOLEAN,
  notes TEXT,
  tags TEXT[]
);

-- Add index for faster queries on common filters
CREATE INDEX idx_command_history_category ON command_history(category_id);
CREATE INDEX idx_command_history_executed_at ON command_history(executed_at);
CREATE INDEX idx_command_history_success ON command_history(success);

-- Create Favorite Commands Table
CREATE TABLE favorite_commands (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  name TEXT NOT NULL,
  command_text TEXT NOT NULL,
  category_id UUID REFERENCES command_categories(id),
  description TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  usage_count INTEGER DEFAULT 0,
  last_used_at TIMESTAMP WITH TIME ZONE,
  tags TEXT[]
);

-- Create Command Patterns Table (for sanitization rules)
CREATE TABLE command_patterns (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  pattern TEXT NOT NULL,
  replacement TEXT NOT NULL,
  description TEXT,
  is_active BOOLEAN DEFAULT TRUE,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Insert some initial sanitization patterns
INSERT INTO command_patterns (pattern, replacement, description) VALUES
('--password=[^ ]+', '--password=***', 'Hide password parameters'),
('-p [^ ]+', '-p ***', 'Hide password after -p flag'),
('token=[a-zA-Z0-9_-]+', 'token=***', 'Hide API tokens'),
('key=[a-zA-Z0-9_-]+', 'key=***', 'Hide API keys'),
('secret=[a-zA-Z0-9_-]+', 'secret=***', 'Hide secrets');

-- Enable RLS on the tables
ALTER TABLE command_history ENABLE ROW LEVEL SECURITY;
ALTER TABLE favorite_commands ENABLE ROW LEVEL SECURITY;
ALTER TABLE command_categories ENABLE ROW LEVEL SECURITY;
ALTER TABLE command_patterns ENABLE ROW LEVEL SECURITY;

-- Create policies (assuming you have a user authentication system)
CREATE POLICY "Users can view their own command history"
  ON command_history FOR SELECT
  USING (auth.uid() = current_setting('app.current_user_id', true)::uuid);

CREATE POLICY "Users can insert their own command history"
  ON command_history FOR INSERT
  WITH CHECK (auth.uid() = current_setting('app.current_user_id', true)::uuid);

CREATE POLICY "Users can view command categories"
  ON command_categories FOR SELECT
  USING (true);

CREATE POLICY "Users can view command patterns"
  ON command_patterns FOR SELECT
  USING (true);

CREATE POLICY "Users can view their favorite commands"
  ON favorite_commands FOR SELECT
  USING (auth.uid() = current_setting('app.current_user_id', true)::uuid);

CREATE POLICY "Users can manage their favorite commands"
  ON favorite_commands FOR ALL
  USING (auth.uid() = current_setting('app.current_user_id', true)::uuid); 

  Here are the fujnctions that got cfreated in supabase

  -- Function to sanitize commands
CREATE OR REPLACE FUNCTION sanitize_command(command_text TEXT)
RETURNS TEXT
LANGUAGE plpgsql
AS $$
DECLARE
  sanitized TEXT := command_text;
  pattern RECORD;
BEGIN
  FOR pattern IN SELECT * FROM command_patterns WHERE is_active = TRUE
  LOOP
    sanitized := regexp_replace(sanitized, pattern.pattern, pattern.replacement, 'g');
  END LOOP;
  
  RETURN sanitized;
END;
$$;

-- Function to get most used commands
CREATE OR REPLACE FUNCTION get_most_used_commands(
  time_period INTERVAL DEFAULT INTERVAL '30 days',
  limit_count INTEGER DEFAULT 10
)
RETURNS TABLE (
  command_text TEXT,
  category_name TEXT,
  usage_count BIGINT,
  success_rate NUMERIC
)
LANGUAGE SQL
AS $$
  SELECT 
    ch.sanitized_command,
    cc.name AS category_name,
    COUNT(*) AS usage_count,
    ROUND(SUM(CASE WHEN ch.success THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100, 2) AS success_rate
  FROM command_history ch
  JOIN command_categories cc ON ch.category_id = cc.id
  WHERE ch.executed_at > NOW() - time_period
  GROUP BY ch.sanitized_command, cc.name
  ORDER BY usage_count DESC
  LIMIT limit_count;
$$;

-- Function to get command usage by category
CREATE OR REPLACE FUNCTION get_command_usage_by_category(
  time_period INTERVAL DEFAULT INTERVAL '30 days'
)
RETURNS TABLE (
  category_name TEXT,
  usage_count BIGINT,
  success_rate NUMERIC
)
LANGUAGE SQL
AS $$
  SELECT 
    cc.name AS category_name,
    COUNT(*) AS usage_count,
    ROUND(SUM(CASE WHEN ch.success THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100, 2) AS success_rate
  FROM command_history ch
  JOIN command_categories cc ON ch.category_id = cc.id
  WHERE ch.executed_at > NOW() - time_period
  GROUP BY cc.name
  ORDER BY usage_count DESC;
$$;

-- Function to get command history with pagination
CREATE OR REPLACE FUNCTION get_command_history(
  category_filter TEXT DEFAULT NULL,
  success_filter BOOLEAN DEFAULT NULL,
  search_term TEXT DEFAULT NULL,
  page_size INTEGER DEFAULT 20,
  page_number INTEGER DEFAULT 1
)
RETURNS TABLE (
  id UUID,
  command_text TEXT,
  sanitized_command TEXT,
  category_name TEXT,
  executed_at TIMESTAMP WITH TIME ZONE,
  duration_ms INTEGER,
  exit_code INTEGER,
  success BOOLEAN,
  notes TEXT,
  tags TEXT[]
)
LANGUAGE SQL
AS $$
  SELECT 
    ch.id,
    ch.command_text,
    ch.sanitized_command,
    cc.name AS category_name,
    ch.executed_at,
    ch.duration_ms,
    ch.exit_code,
    ch.success,
    ch.notes,
    ch.tags
  FROM command_history ch
  JOIN command_categories cc ON ch.category_id = cc.id
  WHERE 
    (category_filter IS NULL OR cc.name = category_filter) AND
    (success_filter IS NULL OR ch.success = success_filter) AND
    (search_term IS NULL OR 
     ch.sanitized_command ILIKE '%' || search_term || '%' OR
     ch.notes ILIKE '%' || search_term || '%')
  ORDER BY ch.executed_at DESC
  LIMIT page_size
  OFFSET (page_number - 1) * page_size;
$$;

-- Function to increment usage count for favorite commands
CREATE OR REPLACE FUNCTION increment_favorite_command_usage(favorite_id UUID)
RETURNS VOID
LANGUAGE plpgsql
AS $$
BEGIN
  UPDATE favorite_commands
  SET 
    usage_count = usage_count + 1,
    last_used_at = NOW()
  WHERE id = favorite_id;
END;
$$;

-- Create a view for command suggestions
CREATE VIEW command_suggestions AS
WITH recent_commands AS (
  SELECT 
    sanitized_command,
    category_id,
    COUNT(*) AS usage_count,
    MAX(executed_at) AS last_used,
    ROUND(SUM(CASE WHEN success THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100, 2) AS success_rate
  FROM command_history
  WHERE executed_at > NOW() - INTERVAL '90 days'
  GROUP BY sanitized_command, category_id
)
SELECT 
  rc.sanitized_command,
  cc.name AS category_name,
  rc.usage_count,
  rc.last_used,
  rc.success_rate,
  CASE 
    WHEN rc.usage_count > 10 AND rc.success_rate > 90 THEN 'high'
    WHEN rc.usage_count > 5 AND rc.success_rate > 70 THEN 'medium'
    ELSE 'low'
  END AS recommendation_strength
FROM recent_commands rc
JOIN command_categories cc ON rc.category_id = cc.id
ORDER BY rc.usage_count DESC, rc.last_used DESC; 

Here is the markdown file that explains about these functions in the database oand ion the front end and the commands and scdripts inolved



# Command History Tracking System

This system tracks and analyzes command execution history, providing insights into command usage patterns, success rates, and suggestions for frequently used commands.

## Features

- **Command History Logging**: Automatically logs executed commands with metadata such as execution time, duration, and exit code
- **Command Sanitization**: Removes sensitive information from commands before storing them
- **Command Categories**: Organizes commands into categories for better organization
- **Favorite Commands**: Save frequently used commands for quick access
- **Command Analytics**: Provides insights into command usage patterns and success rates
- **Command Suggestions**: Suggests commands based on usage patterns and success rates

## Database Structure

The system uses the following tables:

- `command_categories`: Stores command categories
- `command_history`: Logs executed commands with metadata
- `favorite_commands`: Stores favorite commands
- `command_patterns`: Defines patterns for sanitizing sensitive information
- `command_suggestions` (view): Provides command suggestions based on usage patterns

## Setup

### 1. Run Database Migrations

Run the following migration files to set up the database schema:

```bash
# Create tables
pnpm supabase migration up 20250601000000_create_command_history_tables.sql

# Create analytics functions
pnpm supabase migration up 20250601000001_create_command_analytics_functions.sql
```

### 2. Install Dependencies

```bash
# From the repository root
pnpm add -w dotenv @supabase/supabase-js
pnpm add -Dw ts-node typescript @types/node
```

### 3. Configure Environment Variables

Create a `.env` file in the repository root with the following variables:

```
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
```

## Usage

### Tracking Commands

Use the `track.sh` script to execute and track commands:

```bash
# Format
./scripts/track.sh [category] [command]

# Examples
./scripts/track.sh git "git push origin main"
./scripts/track.sh pnpm "pnpm install marked"
```

### Setting Up Command Aliases

Add the following aliases to your `.bashrc` or `.zshrc` file for easier command tracking:

```bash
# Replace with the actual path to your track.sh script
alias tgit='~/path/to/scripts/track.sh git'
alias tpnpm='~/path/to/scripts/track.sh pnpm'
alias tbuild='~/path/to/scripts/track.sh build'
alias tdeploy='~/path/to/scripts/track.sh deploy'
alias tdb='~/path/to/scripts/track.sh database'
alias tsys='~/path/to/scripts/track.sh system'
alias tother='~/path/to/scripts/track.sh other'
```

Then use them like:

```bash
tgit "git push origin main"
tpnpm "pnpm install marked"
```

## TypeScript Service

The `commandHistoryService.ts` provides methods for interacting with the command history system:

```typescript
// Import the service
import { CommandHistoryService } from '../services/commandHistoryService';

// Create an instance
const commandHistory = new CommandHistoryService();

// Record a command
await commandHistory.recordCommand(
  'git push origin main',
  'git',
  0,
  1500,
  'Pushed changes to main branch',
  ['deployment', 'git']
);

// Get command history
const history = await commandHistory.getCommandHistory({
  categoryFilter: 'git',
  successFilter: true,
  searchTerm: 'push',
  pageSize: 10,
  pageNumber: 1
});

// Get favorite commands
const favorites = await commandHistory.getFavoriteCommands();

// Get command suggestions
const suggestions = await commandHistory.getCommandSuggestions();

// Get most used commands
const mostUsed = await commandHistory.getMostUsedCommands('30 days', 10);

// Get command usage by category
const categoryUsage = await commandHistory.getCommandUsageByCategory('30 days');
```

## Analytics Functions

The system provides several analytics functions:

- `sanitize_command`: Sanitizes command text based on patterns
- `get_most_used_commands`: Gets the most used commands within a time period
- `get_command_usage_by_category`: Gets command usage statistics by category
- `get_command_history`: Gets command history with filtering and pagination
- `increment_favorite_command_usage`: Increments usage count for a favorite command

## Command Sanitization

The system sanitizes commands to remove sensitive information before storing them. Add patterns to the `command_patterns` table to define what should be sanitized:

```sql
INSERT INTO command_patterns (pattern, replacement, is_active, description)
VALUES 
  ('password=\w+', 'password=***', true, 'Hide passwords'),
  ('token=\w+', 'token=***', true, 'Hide tokens'),
  ('key=\w+', 'key=***', true, 'Hide keys');
```

## Security

The system uses Row Level Security (RLS) to ensure that users can only access their own command history and favorite commands. The following policies are applied:

- Users can only view their own command history
- Users can only manage their own favorite commands
- Command categories and patterns are accessible to all authenticated users 



Make a new page called AI
It will be a dashboard similar to the other dashboards you've built for me. 
Its purpose is to help me write sophisticated prompts with sonnet 3.7 
Here is what is working well for me that I need the dashboard to support:
1) similar to the "Projects" of claude pro - I need to select quite a few files that give it context for building a prompt that will assist the ai in writing a very detailed prompt I then save as a markdown file (hopefully in my root directory) in the prompts folder that is this prompt in markdown format.  Sometimes dedicated JSON to fill the results for the ai is provided to the prompt to help it fill and make sense of unstrcutred data that it extracts from the file content.  I then load that up into a query to the claude sonnet 3.7 api and apply the prompt to a a file or one or more files.  The code that this is wrapped in is usually a functioh on one of my pages that retrieves the content, applies the prompt using the claude api and retrieveds the json data and stores it in an experts_document records in the processed_content field
2) something that lists all the prompts based on their filenames - and maybe has the last associated date that the file was updated - but sopmethjing simople based on the stored file metadata on my local project.  This sits on top of a simple markdown viewer
3) Other functions I need - a markdown viewer that will allow me to read the prompts. The viewer will alway be reading from a local version of the markdown file that is stored in my root docs folder in some subfolder - for now
4) Other functions you can think of that would be helpful in using the claude api for sonnet 3.7 - for example - other properties such as temperature (whicih I almosrt always make 0 as I don't want the ai to make anything up).  These settings would be stored in localsttorage so the values would persist over multiple sessionhs (or eventually perhaps in the database - but not yet)
Here are some paths to prompts I am buidling or have built so far: docs/prompts/code-analysis-prompt.md
docs/prompts/document-classification-prompt.md
docs/prompts/react-component-analysis-prompt.md
docs/prompts/expert-extraction-prompt.md
These prompt are very effective in extracting the content from the source files they are applied to.
One more thing to consider is that applying the prompts will be based most likely on what document type a file is classified as - below is what the json is for that table.  This should somehow be integrated into your design.  In the future there will be a relationship between the prompt and the document type - I will probably be building them out by document type - so now that I think about it if you listed the files by document types - I could select a certain of number of them to improve the prompt generation.  In fact as I build out new prompts I may want to create a new document type (problably of json mime type - and then I can apply that to the expert_documents after the ai processing on the content has finished - perhaps I can then even view prompts by input and output document tyes)

-- types of primary tables in the database involved in prompts extraction are shown below

{
  "document_types": {
    "Row": {
      "ai_processing_rules": "Json | null",
      "category": "string",
      "content_schema": "Json | null",
      "created_at": "string",
      "current_num_of_type": "number | null",
      "description": "string | null",
      "document_type": "string",
      "document_type_counts": "number | null",
      "file_extension": "string | null",
      "id": "string",
      "is_ai_generated": "boolean",
      "legacy_document_type_id": "number | null",
      "mime_type": "string | null",
      "required_fields": "Json | null",
      "updated_at": "string",
      "validation_rules": "Json | null"
    },
    "Relationships": []
  }
}

{
  "sources_google": {
    "Row": {
      "created_at": "string",
      "created_by": "string | null",
      "drive_id": "string",
      "file_extension": "string | null",
      "file_hash": "string | null",
      "file_size": "number | null",
      "folder_path": "string | null",
      "id": "string",
      "is_folder": "boolean | null",
      "is_google_doc": "boolean | null",
      "is_google_sheet": "boolean | null",
      "is_google_slide": "boolean | null",
      "is_processed": "boolean | null",
      "is_trashed": "boolean | null",
      "last_modified": "string | null",
      "last_modified_by": "string | null",
      "mime_type": "string | null",
      "name": "string",
      "parent_folder_id": "string | null",
      "processing_error": "string | null",
      "processing_status": "string | null",
      "shared_with": "string[] | null",
      "updated_at": "string",
      "updated_by": "string | null",
      "web_view_link": "string | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "sources_google_parent_folder_id_fkey",
        "columns": ["parent_folder_id"],
        "isOneToOne": false,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "expert_documents": {
    "Row": {
      "ai_analysis": "Json | null",
      "ai_processing_details": "Json | null",
      "batch_id": "string | null",
      "classification_confidence": "number | null",
      "classification_metadata": "Json | null",
      "confidence_score": "number | null",
      "content_type": "string | null",
      "created_at": "string",
      "diarization_complete": "boolean | null",
      "document_type_id": "string | null",
      "error_message": "string | null",
      "expert_id": "string | null",
      "id": "string",
      "is_latest": "boolean | null",
      "key_insights": "string[] | null",
      "language": "string | null",
      "last_error_at": "string | null",
      "last_processed_at": "string | null",
      "last_viewed_at": "string | null",
      "model_used": "string | null",
      "previous_version_id": "string | null",
      "processed_at": "string | null",
      "processed_content": "Json | null",
      "processing_completed_at": "string | null",
      "processing_error": "string | null",
      "processing_started_at": "string | null",
      "processing_stats": "Json | null",
      "processing_status": "string | null",
      "prompt_used": "string | null",
      "queued_at": "string | null",
      "raw_content": "string | null",
      "retry_count": "number | null",
      "source_id": "string",
      "status": "string | null",
      "structure": "Json | null",
      "summary_complete": "boolean | null",
      "token_count": "number | null",
      "topics": "string[] | null",
      "transcription_complete": "boolean | null",
      "updated_at": "string",
      "version": "number | null",
      "whisper_model_used": "string | null",
      "word_count": "number | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "expert_documents_batch_id_fkey",
        "columns": ["batch_id"],
        "isOneToOne": false,
        "referencedRelation": "batch_processing_status",
        "referencedColumns": ["batch_id"]
      },
      {
        "foreignKeyName": "expert_documents_batch_id_fkey",
        "columns": ["batch_id"],
        "isOneToOne": false,
        "referencedRelation": "processing_batches",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_document_type_id_fkey",
        "columns": ["document_type_id"],
        "isOneToOne": false,
        "referencedRelation": "document_types",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_expert_id_fkey",
        "columns": ["expert_id"],
        "isOneToOne": false,
        "referencedRelation": "experts",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_previous_version_id_fkey",
        "columns": ["previous_version_id"],
        "isOneToOne": false,
        "referencedRelation": "expert_documents",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_source_id_fkey",
        "columns": ["source_id"],
        "isOneToOne": false,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  }
}








Make a new page called Show.



It will NOT be a dashobard like the others, but rather built around the goal of helping users navigate, view and learn from related pdf and docx and ai documents associated with them.  So the viewer is only one component of this page.  It should look good.
It is ultimately going to be a prototype of a dedciated application I will be building.
It involves these 10 tables I am giving you the json for that are already in the datgabase but empty.
The presentations table is the most important as there will be one for every MP4 we have in the system - which now is about 166.  This is the presentation we are building context around - so all of these assets are files that exist mostly on the google drive and are represented in the sources_google file.  In addition most of these files will have been processed by ai or other means and their json or summaries will be in the expert_documents table. 
I am including the path to apps/dhg-improve-experts/src/components/FileViewer.tsx which is a file viewer that uses the drive extracted from the web link in sources_google to view the file using google reader.  It will be an integral part of your solution as ultimately we will be displaying the presetnatiopn.  But how we search for them, use the tagged assets and other presentations to give context (meaning related docx and pdf files and ai summaries associated with them) - that's part of what we want you to come up with.  
Feel free to use the real mp4s, but after that you can mock up the objects that will be represented ihn the database tables, but you do not yet have to interact much with the database.  We just want to explore what it might look life before we go hooking things up.


I am including the the exact types from the 10 tables plus expert_documents and sources_google below.

{
  "presentation_assets": {
    "Row": {
      "asset_role": "asset_role_enum | null",
      "asset_type": "asset_type_enum | null",
      "asset_type_id": "string | null",
      "created_at": "string",
      "expert_document_id": "string | null",
      "id": "string",
      "importance_level": "number | null",
      "metadata": "Json | null",
      "presentation_id": "string | null",
      "source_id": "string | null",
      "timestamp_end": "number | null",
      "timestamp_start": "number | null",
      "updated_at": "string",
      "user_notes": "string | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_assets_asset_type_id_fkey",
        "columns": ["asset_type_id"],
        "referencedRelation": "asset_types",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_assets_expert_document_id_fkey",
        "columns": ["expert_document_id"],
        "referencedRelation": "expert_documents",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_assets_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "presentations",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_assets_source_id_fkey",
        "columns": ["source_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_collection_items": {
    "Row": {
      "collection_id": "string",
      "created_at": "string | null",
      "notes": "string | null",
      "position": "number",
      "presentation_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_collection_items_collection_id_fkey",
        "columns": ["collection_id"],
        "referencedRelation": "presentation_collections",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_collection_items_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_collections": {
    "Row": {
      "created_at": "string | null",
      "description": "string | null",
      "id": "string",
      "is_public": "boolean | null",
      "name": "string",
      "updated_at": "string | null"
    },
    "Relationships": []
  },
  "presentation_relationships": {
    "Row": {
      "created_at": "string | null",
      "relationship_type": "string",
      "source_presentation_id": "string",
      "strength": "number | null",
      "target_presentation_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_relationships_source_presentation_id_fkey",
        "columns": ["source_presentation_id"],
        "referencedRelation": "presentations",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_relationships_target_presentation_id_fkey",
        "columns": ["target_presentation_id"],
        "referencedRelation": "presentations",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_search_index": {
    "Row": {
      "content_vector": "unknown | null",
      "presentation_id": "string",
      "title_vector": "unknown | null",
      "updated_at": "string | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_search_index_presentation_id_fkey",
        "columns": ["presentation_id"],
        "isOneToOne": true,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_tag_links": {
    "Row": {
      "created_at": "string | null",
      "presentation_id": "string",
      "tag_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_tag_links_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_tag_links_tag_id_fkey",
        "columns": ["tag_id"],
        "referencedRelation": "presentation_tags",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_tags": {
    "Row": {
      "color": "string | null",
      "created_at": "string | null",
      "id": "string",
      "name": "string"
    },
    "Relationships": []
  },
  "presentation_theme_links": {
    "Row": {
      "created_at": "string | null",
      "presentation_id": "string",
      "relevance_score": "number | null",
      "theme_id": "string"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentation_theme_links_presentation_id_fkey",
        "columns": ["presentation_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "presentation_theme_links_theme_id_fkey",
        "columns": ["theme_id"],
        "referencedRelation": "presentation_themes",
        "referencedColumns": ["id"]
      }
    ]
  },
  "presentation_themes": {
    "Row": {
      "ai_confidence": "number | null",
      "created_at": "string | null",
      "description": "string | null",
      "id": "string",
      "name": "string"
    },
    "Relationships": []
  },
  "presentations": {
    "Row": {
      "created_at": "string | null",
      "duration": "unknown | null",
      "duration_seconds": "number | null",
      "filename": "string",
      "folder_path": "string",
      "id": "string",
      "is_public": "boolean | null",
      "main_video_id": "string | null",
      "metadata": "Json | null",
      "presenter_name": "string | null",
      "recorded_date": "string | null",
      "title": "string | null",
      "transcript": "string | null",
      "transcript_status": "string | null",
      "updated_at": "string | null",
      "view_count": "number | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "presentations_main_video_id_fkey",
        "columns": ["main_video_id"],
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  }
}

Here are the types for the two crucial tables that the presentation tables are built on top of: 
{
  "expert_documents": {
    "Row": {
      "ai_analysis": "Json | null",
      "ai_processing_details": "Json | null",
      "batch_id": "string | null",
      "classification_confidence": "number | null",
      "classification_metadata": "Json | null",
      "confidence_score": "number | null",
      "content_type": "string | null",
      "created_at": "string",
      "diarization_complete": "boolean | null",
      "document_type_id": "string | null",
      "error_message": "string | null",
      "expert_id": "string | null",
      "id": "string",
      "is_latest": "boolean | null",
      "key_insights": "string[] | null",
      "language": "string | null",
      "last_error_at": "string | null",
      "last_processed_at": "string | null",
      "last_viewed_at": "string | null",
      "model_used": "string | null",
      "previous_version_id": "string | null",
      "processed_at": "string | null",
      "processed_content": "Json | null",
      "processing_completed_at": "string | null",
      "processing_error": "string | null",
      "processing_started_at": "string | null",
      "processing_stats": "Json | null",
      "processing_status": "string | null",
      "prompt_used": "string | null",
      "queued_at": "string | null",
      "raw_content": "string | null",
      "retry_count": "number | null",
      "source_id": "string",
      "status": "string | null",
      "structure": "Json | null",
      "summary_complete": "boolean | null",
      "token_count": "number | null",
      "topics": "string[] | null",
      "transcription_complete": "boolean | null",
      "updated_at": "string",
      "version": "number | null",
      "whisper_model_used": "string | null",
      "word_count": "number | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "expert_documents_batch_id_fkey",
        "columns": ["batch_id"],
        "isOneToOne": false,
        "referencedRelation": "batch_processing_status",
        "referencedColumns": ["batch_id"]
      },
      {
        "foreignKeyName": "expert_documents_batch_id_fkey",
        "columns": ["batch_id"],
        "isOneToOne": false,
        "referencedRelation": "processing_batches",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_document_type_id_fkey",
        "columns": ["document_type_id"],
        "isOneToOne": false,
        "referencedRelation": "document_types",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_expert_id_fkey",
        "columns": ["expert_id"],
        "isOneToOne": false,
        "referencedRelation": "experts",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_previous_version_id_fkey",
        "columns": ["previous_version_id"],
        "isOneToOne": false,
        "referencedRelation": "expert_documents",
        "referencedColumns": ["id"]
      },
      {
        "foreignKeyName": "expert_documents_source_id_fkey",
        "columns": ["source_id"],
        "isOneToOne": false,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  },
  "sources_google": {
    "Row": {
      "created_at": "string",
      "created_by": "string | null",
      "drive_id": "string",
      "file_extension": "string | null",
      "file_hash": "string | null",
      "file_size": "number | null",
      "folder_path": "string | null",
      "id": "string",
      "is_folder": "boolean | null",
      "is_google_doc": "boolean | null",
      "is_google_sheet": "boolean | null",
      "is_google_slide": "boolean | null",
      "is_processed": "boolean | null",
      "is_trashed": "boolean | null",
      "last_modified": "string | null",
      "last_modified_by": "string | null",
      "mime_type": "string | null",
      "name": "string",
      "parent_folder_id": "string | null",
      "processing_error": "string | null",
      "processing_status": "string | null",
      "shared_with": "string[] | null",
      "updated_at": "string",
      "updated_by": "string | null",
      "web_view_link": "string | null"
    },
    "Relationships": [
      {
        "foreignKeyName": "sources_google_parent_folder_id_fkey",
        "columns": ["parent_folder_id"],
        "isOneToOne": false,
        "referencedRelation": "sources_google",
        "referencedColumns": ["id"]
      }
    ]
  }
}



Make a new page called "Docs".

-- claude code:
1. Fixed the Supabase import path in Docs.tsx
  2. Modified database query methods to work even if the documentation tables
  don't exist yet
  3. Added mock data and error handling for file tree display
  4. Improved file content loading with fallbacks to local content
  5. Created a comprehensive script (process-documentation.ts) to help manage
  documentation files

  The Docs dashboard page is now ready to use and includes:
  - File tree navigation
  - Markdown content viewer
  - Search functionality
  - Tag-based filtering
  - Document statistics

  The script in scripts/process-documentation.ts helps solve your problems by:
  1. Scanning and consolidating markdown files from different locations
  2. Managing prompt files between apps and public folders
  3. Tracking document metadata including creation/modification dates
  4. Creating a central database for all documentation

  You can start using the page at http://localhost:5174/docs




It will be a dashboard similar to the other dashboards you've built for me. 
Its purpose is to help  me manage my markdown documentation files that are in my docs folder in the root of the mono repo.  Here's some of the problems it should help me with
1) many new markdown files are created and they are all over the place - in the apps folder, in the root folder, in other folders - I need to consoldiate the strays
2) my prompt markdown files.  These are very important files as they are loaded and applied to my code to guide ai sessions.  The obvious choice wouold be to have the prompts folder be the only source of truth for these files, but I found out that they needed to be with an apps folder in the public folder for it to work, so on therre is a symlink to the public file in my local apps prompt folder. I need a better way to handle this as it makes having a source of truth difficult
3) then there is the problem of know when the markdown file was created - for that I have created these objects in sql to help manage the process - which you can use in your solution
4) I have settled on a hybrid approach to handling the markdown docs
First you'll see the database metadata I hope to fill - below
Second you have the database functions that have been created that work with these tables
Third - you have instrudtions below here that describes the hybrid approach - note that I have installed the 2 depencides of "marked" and "crypto" that the code depends on
Fourth - hhere is the link to apps/dhg-improve-experts/src/services/documentationService.ts which contains the matching front end code
Fifth - I am sharing the script utility path that is designed to drive the updates: scripts/process-documentation.ts
Sixth - this docs folder will need a markdown viewer integrated into it that will allow me to view the nicely formatted markdown files and there needs to be a tree type of control that will help me choose the file to view easily
Seventh there needs to be a search window that will narrow down the choices of what to view
Eighth - just having the dagte of the update or creation of the markdown file in some ui based element could be very helpful because then I could use history to find them - or a combination
Okay - lets see what you create on these specs


---------- HERE ARE THE VARIOUS OBJECTS INVOLVED - IF THAT HELPS
Summary of Implementation
I've implemented a comprehensive documentation management system according to your requirements. Here's what I've created:
Database Structure
Migration Files:
Created SQL migration files for tables, functions, and search capabilities
Included corresponding down migrations for each file
These files are ready to be run in the Supabase UI
Backend Implementation
DocumentationService:
Created a TypeScript service that handles:
Processing markdown files (extracting frontmatter, sections)
Registering files in the database
Generating summaries and tags (with placeholder AI functionality)
Detecting relationships between documents
Searching and retrieving documentation
Command-Line Tool:
Created a script to process documentation files
Supports scanning directories, processing individual files, and batch processing
Documentation
Usage Guide:
Created a comprehensive markdown guide explaining the system
Includes instructions for setup, usage, and future enhancements
Key Features
Metadata Tracking: Stores metadata about markdown files without modifying them
AI Integration: Placeholder implementation for AI-powered summaries and tagging
Search Functionality: Full-text search with relevance ranking
Relationship Detection: Identifies links between documents
Section Navigation: Tracks document structure for easy navigation
Next Steps
As requested, I've focused on the database structure and backend implementation without modifying the UI. The next steps would be:
Run the migration files in the Supabase UI
Install the required dependencies (marked and crypto)
Create a docs directory and add some markdown files
Run the processing script to populate the database
Implement a UI for browsing and searching documentation
Would you like me to make any adjustments to the implementation or explain any part in more detail?



here are the tables and objects created for managing the document files as metadata
-- Migration: Create Documentation Management System Tables
-- Description: Sets up tables for tracking markdown documentation files and their metadata

-- Documentation files table - stores metadata about markdown files
CREATE TABLE documentation_files (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_path TEXT UNIQUE NOT NULL,
  title TEXT NOT NULL,
  summary TEXT,
  ai_generated_tags TEXT[],
  manual_tags TEXT[],
  last_modified_at TIMESTAMPTZ NOT NULL,
  last_indexed_at TIMESTAMPTZ NOT NULL,
  file_hash TEXT, -- For detecting changes
  metadata JSONB, -- Extracted frontmatter, headings, etc.
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Documentation relations table - tracks relationships between documents
CREATE TABLE documentation_relations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  source_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  target_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  relation_type TEXT NOT NULL, -- 'reference', 'related', etc.
  created_at TIMESTAMPTZ DEFAULT now(),
  -- Prevent duplicate relations
  CONSTRAINT unique_documentation_relation UNIQUE (source_id, target_id, relation_type)
);

-- Documentation sections table - tracks sections within documents
CREATE TABLE documentation_sections (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  heading TEXT NOT NULL,
  level INTEGER NOT NULL, -- h1, h2, etc.
  position INTEGER NOT NULL, -- Order in document
  anchor_id TEXT NOT NULL, -- For direct linking
  summary TEXT,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now(),
  -- Ensure unique anchors within a document
  CONSTRAINT unique_section_anchor UNIQUE (file_id, anchor_id)
);

-- Documentation processing queue - tracks files that need AI processing
CREATE TABLE documentation_processing_queue (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID REFERENCES documentation_files(id) ON DELETE CASCADE NOT NULL,
  status TEXT NOT NULL DEFAULT 'pending', -- 'pending', 'processing', 'completed', 'failed'
  priority INTEGER NOT NULL DEFAULT 1,
  attempts INTEGER NOT NULL DEFAULT 0,
  last_attempt_at TIMESTAMPTZ,
  error_message TEXT,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now()
);

-- Create indexes for better performance
CREATE INDEX idx_documentation_files_path ON documentation_files (file_path);
CREATE INDEX idx_documentation_files_tags ON documentation_files USING GIN (ai_generated_tags, manual_tags);
CREATE INDEX idx_documentation_sections_file_id ON documentation_sections (file_id);
CREATE INDEX idx_documentation_processing_queue_status ON documentation_processing_queue (status, priority);

-- Create a function to update the updated_at timestamp
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
   NEW.updated_at = now();
   RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create triggers to automatically update the updated_at column
CREATE TRIGGER update_documentation_files_updated_at
BEFORE UPDATE ON documentation_files
FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_documentation_sections_updated_at
BEFORE UPDATE ON documentation_sections
FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_documentation_processing_queue_updated_at
BEFORE UPDATE ON documentation_processing_queue
FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- Create a function to add a file to the processing queue
CREATE OR REPLACE FUNCTION queue_documentation_file_for_processing(file_id UUID, priority INTEGER DEFAULT 1)
RETURNS UUID AS $$
DECLARE
  queue_id UUID;
BEGIN
  -- Check if file is already in queue
  SELECT id INTO queue_id FROM documentation_processing_queue 
  WHERE file_id = queue_documentation_file_for_processing.file_id AND status IN ('pending', 'processing');
  
  IF queue_id IS NULL THEN
    -- Add to queue if not already there
    INSERT INTO documentation_processing_queue (file_id, priority)
    VALUES (file_id, priority)
    RETURNING id INTO queue_id;
  ELSE
    -- Update priority if already in queue
    UPDATE documentation_processing_queue
    SET priority = GREATEST(priority, queue_documentation_file_for_processing.priority)
    WHERE id = queue_id;
  END IF;
  
  RETURN queue_id;
END;
$$ LANGUAGE plpgsql;

here are the sql fuunctions available:
-- Migration: Create Documentation Processing Functions
-- Description: Functions for processing markdown files and managing documentation metadata

-- Function to extract a filename from a path
CREATE OR REPLACE FUNCTION extract_filename(file_path TEXT)
RETURNS TEXT AS $$
BEGIN
  RETURN substring(file_path from '([^/]+)(?:\.[^.]+)?$');
END;
$$ LANGUAGE plpgsql;

-- Function to register a markdown file in the system
CREATE OR REPLACE FUNCTION register_markdown_file(
  p_file_path TEXT,
  p_title TEXT DEFAULT NULL,
  p_file_hash TEXT DEFAULT NULL,
  p_metadata JSONB DEFAULT '{}'::JSONB
)
RETURNS UUID AS $$
DECLARE
  v_file_id UUID;
  v_title TEXT;
BEGIN
  -- Determine title if not provided
  IF p_title IS NULL THEN
    v_title := extract_filename(p_file_path);
  ELSE
    v_title := p_title;
  END IF;

  -- Check if file already exists
  SELECT id INTO v_file_id FROM documentation_files WHERE file_path = p_file_path;
  
  IF v_file_id IS NULL THEN
    -- Insert new file record
    INSERT INTO documentation_files (
      file_path,
      title,
      last_modified_at,
      last_indexed_at,
      file_hash,
      metadata
    ) VALUES (
      p_file_path,
      v_title,
      now(),
      now(),
      p_file_hash,
      p_metadata
    )
    RETURNING id INTO v_file_id;
  ELSE
    -- Update existing file record
    UPDATE documentation_files
    SET
      title = v_title,
      last_modified_at = now(),
      last_indexed_at = now(),
      file_hash = COALESCE(p_file_hash, file_hash),
      metadata = COALESCE(p_metadata, metadata)
    WHERE id = v_file_id;
  END IF;
  
  -- Queue for AI processing
  PERFORM queue_documentation_file_for_processing(v_file_id);
  
  RETURN v_file_id;
END;
$$ LANGUAGE plpgsql;

-- Function to register a section within a document
CREATE OR REPLACE FUNCTION register_document_section(
  p_file_id UUID,
  p_heading TEXT,
  p_level INTEGER,
  p_position INTEGER,
  p_anchor_id TEXT,
  p_summary TEXT DEFAULT NULL
)
RETURNS UUID AS $$
DECLARE
  v_section_id UUID;
BEGIN
  -- Check if section already exists
  SELECT id INTO v_section_id 
  FROM documentation_sections 
  WHERE file_id = p_file_id AND anchor_id = p_anchor_id;
  
  IF v_section_id IS NULL THEN
    -- Insert new section
    INSERT INTO documentation_sections (
      file_id,
      heading,
      level,
      position,
      anchor_id,
      summary
    ) VALUES (
      p_file_id,
      p_heading,
      p_level,
      p_position,
      p_anchor_id,
      p_summary
    )
    RETURNING id INTO v_section_id;
  ELSE
    -- Update existing section
    UPDATE documentation_sections
    SET
      heading = p_heading,
      level = p_level,
      position = p_position,
      summary = COALESCE(p_summary, summary)
    WHERE id = v_section_id;
  END IF;
  
  RETURN v_section_id;
END;
$$ LANGUAGE plpgsql;

-- Function to register a relationship between documents
CREATE OR REPLACE FUNCTION register_document_relation(
  p_source_id UUID,
  p_target_id UUID,
  p_relation_type TEXT
)
RETURNS UUID AS $$
DECLARE
  v_relation_id UUID;
BEGIN
  -- Check if relation already exists
  SELECT id INTO v_relation_id 
  FROM documentation_relations 
  WHERE source_id = p_source_id AND target_id = p_target_id AND relation_type = p_relation_type;
  
  IF v_relation_id IS NULL THEN
    -- Insert new relation
    INSERT INTO documentation_relations (
      source_id,
      target_id,
      relation_type
    ) VALUES (
      p_source_id,
      p_target_id,
      p_relation_type
    )
    RETURNING id INTO v_relation_id;
  END IF;
  
  RETURN v_relation_id;
END;
$$ LANGUAGE plpgsql;

-- Function to update document AI-generated metadata
CREATE OR REPLACE FUNCTION update_document_ai_metadata(
  p_file_id UUID,
  p_summary TEXT,
  p_ai_generated_tags TEXT[]
)
RETURNS VOID AS $$
BEGIN
  UPDATE documentation_files
  SET
    summary = p_summary,
    ai_generated_tags = p_ai_generated_tags
  WHERE id = p_file_id;
  
  -- Mark processing as complete
  UPDATE documentation_processing_queue
  SET
    status = 'completed',
    updated_at = now()
  WHERE file_id = p_file_id AND status = 'processing';
END;
$$ LANGUAGE plpgsql;

-- Function to get the next file for AI processing
CREATE OR REPLACE FUNCTION get_next_file_for_processing()
RETURNS TABLE (
  queue_id UUID,
  file_id UUID,
  file_path TEXT
) AS $$
BEGIN
  RETURN QUERY
  WITH next_file AS (
    SELECT 
      dpq.id as queue_id,
      dpq.file_id,
      df.file_path
    FROM documentation_processing_queue dpq
    JOIN documentation_files df ON dpq.file_id = df.id
    WHERE dpq.status = 'pending'
    ORDER BY dpq.priority DESC, dpq.created_at ASC
    LIMIT 1
    FOR UPDATE SKIP LOCKED
  )
  UPDATE documentation_processing_queue dpq
  SET 
    status = 'processing',
    attempts = attempts + 1,
    last_attempt_at = now()
  FROM next_file
  WHERE dpq.id = next_file.queue_id
  RETURNING next_file.queue_id, next_file.file_id, next_file.file_path;
END;
$$ LANGUAGE plpgsql;



On the sync page, under the folders tab you can now choose different folders to sync.  This introduces the concept of "current sync folder" which now the sync dashboard needs to reflect everywhere.  Under the folders tab you can now choose which folder you want to sync - and even add new folders - whatever is last synced is the current folder. But perhaps you need to provide a way to select which folder you want to see the statistics for in the sync dashboard and then keep the statistics for that current folder showing the relevant facts for that folder.  

 keeping the guts tables and architecture in mind, provide a way on the 
  code dashboard to analyze all the functions on the viewer page and to add
   them to the function_registry function - with proper formatting, you can
   see one of my attempts in the markdown file: 
  apps/dhg-improve-experts/public/prompts/enhanced-analysis-prompt.md and 
  there are others in docs/prompts/react-component-analysis-prompt.md that 
  tried to detailed ihnformtaionh about the functions. 
  docs/prompts/enhanced-analysis-prompt.md - the main point of these was 
  that when a function gets added to the function_registry we need to 
  classify it according to whether it is for the dashboard, whether it is a
   candidate for further refactoring because it could be part of a utils 
  folder, we need the associatioh to the page it is in, we need to know if 
  is a react function - if you need to use one of these prompts or create a
   new one to help a sophisticated analysis, please reinstate one or more 
  of these prompts to assist you.  Remeber then these functions in the 
  function registry will be available to the guts tables 


Move the Analyze and Registry page and functionality to the code dashboard page as new items across the top and display their respetive ui objects you did before only now under the code page tabs



Make a new page called "Write".
It will be a dashboard similar to the other dashboard you've built for me. 
Its purpose is help ai process a set of related research documents to build compelling summaries from multiple pdfs. It needs to let you choose a folder and or a specific pdf as your primary document and then help you find the related pdfs you wish to associate with the primary docuemnt and an ability to launch an ai prompt to interact with these documents to produce a new summary output.  It should take into account document_types for each of these documents. Lets see what you come up to meet these goals.



Make a new page called "Code" 
It will be a dashboard similar to the other dashboard you've built for me.
It needs to manage the following tasks and any others you think are neceessary
It will support a function registry that is already in the database - it has vital information (gathered and formatted by ai) about each function. it should classify functions whether they are local to a dashboard, or utility based and therefore candiates for refacotring in servcies, functions should be associated with the pages that call them
It should have something about the organization of the app - particularly about the pages on the mainnavbar which is the skeleton of the program.




add an addtional tab on the experts page called profliles. This should display the content from the processed_content field which has json describing various items about an expert.  Format each record on a card and make the json output pretty.


## original code
I need you to add a new experts page soon.  But before you do that I need you to sysematically go through my code and find references to previous work on experts - I think can just search for any thing that has the word expert in it.  I believe there are a lot of orphaned functions and ui dealing with example calls and previous attempts at managing experts. I will be wanting to archive and get rid of those.

To help me do so  Generate a thorough report in markdown syntax that I can put into my docs folder that will assess all the experts code and make suggestikons about wht is needed and what can be removed - and even provide commands to do so I can paste into my terminal


Now that I've cleaned up the experts code I need you to add a new experts page.  It will be a dashboard that will allow me to manage experts similar to all the other dashboards you created for me. I have an existing experts table and I have an existing experts_documents table.  I need to be able to add new experts, edit their information, and delete them.  I will also need to be able to see a list of all experts and select one to view their details.  I will also need to be able to add new expert documents and edit their information, and delete them.  I will also need to be able to see a list of all expert documents and select one to view their details.

The thing about experts is the the information for them comes from many documents. Our goal is to build up and keep current important information about them and even update it periodically. We already have an "experts" table but we need to make it more robust.  Please add the fields you think are neceessary to do this.

Some of our information comes from the presentation announcement docx files which we have in our sources_google table.  We are processing with ai and extracting out the unstructured information that comes from presentation documents that are cvs and bios.  Some of it comes from their research papers. Some of it could come from the web sites that are extracted from some of these documents such as their lab page. Yes, we could even get their linked in profiles.  Basically the experts who create our content videos are the heart of our operation and keeping up to date information about them is crucial.  We also will be building an associated set of tables and ai processing around their research papers and content but that will be a later step.

  We have to take all these sources and extract out the unstructureed (but mostly similar and consistent information ) and put it into the experts table.  We also need to be able to add new experts and edit their information, and delete them.  I will also need to be able to see a list of all experts and select one to view their details.  I will also need to be able to add new expert documents and edit their information, and delete those files. 

  here are the fields in the current experts table

   experts: {
        Row: {
          bio: string | null
          created_at: string
          email_address: string | null
          experience_years: number | null
          expert_name: string
          expertise_area: string | null
          full_name: string | null
          google_email: string | null
          google_profile_data: Json | null
          google_user_id: string | null
          id: string
          is_in_core_group: boolean
          last_synced_at: string | null
          legacy_expert_id: number | null
          starting_ref_id: number | null
          sync_error: string | null
          sync_status: string | null
          updated_at: string
          user_id: string | null
        }
        Insert: {
          bio?: string | null
          created_at?: string
          email_address?: string | null
          experience_years?: number | null
          expert_name: string
          expertise_area?: string | null
          full_name?: string | null
          google_email?: string | null
          google_profile_data?: Json | null
          google_user_id?: string | null
          id?: string
          is_in_core_group?: boolean
          last_synced_at?: string | null
          legacy_expert_id?: number | null
          starting_ref_id?: number | null
          sync_error?: string | null
          sync_status?: string | null
          updated_at?: string
          user_id?: string | null
        }
        Update: {
          bio?: string | null
          created_at?: string
          email_address?: string | null
          experience_years?: number | null
          expert_name?: string
          expertise_area?: string | null
          full_name?: string | null
          google_email?: string | null
          google_profile_data?: Json | null
          google_user_id?: string | null
          id?: string
          is_in_core_group?: boolean
          last_synced_at?: string | null
          legacy_expert_id?: number | null
          starting_ref_id?: number | null
          sync_error?: string | null
          sync_status?: string | null
          updated_at?: string
          user_id?: string | null
        }
        Relationships: []
      }




I need you to improve the supabase page.  It is all about the supabase design and how we can improve it to make it more efficient and easier to use.  It needs to be more intuitive and follow the steps required to get the job done.  

Just as you did for now three other pages - classify, sync and transcribe - I need you to do the same for the supabase page.  I want to see all the tables and sql objects that a person managing a sophisticated postgres database needs to know - what we currently have, what is missing or inconsistent, what needs to be added and what needs to be removed. 

I want to see a summary of the tables and their current status and then be able to generate objects such as tables, views, enums, etc. that will make the database more efficient and easier to use.

I need to be able a section that will allow me to manage migrations better - but it doesn't have to follow the traditional up and down path.  It seems I like to have you first generate the commands based on mp prompts and then paste them in the supabase ai and to use the SQL editor to run them.  Usually a mistake in the sql crops up and I paste it into you to troubleshoot and run it until it is successful. It's hard to keep track of that using the traditional up and down migrations, plus the final version of the code that actually worked is usually not in the final migration sql.

Also, I like to generate new versions of the schema and all database objects to paste into the coding ai to help it write better code.  Then in the terminal I regularly export all the types and hand off the current types.ts file to the ai to help it write better code.  My local command always generates the types.ts file in supabase/types.ts.   

Try to keep existing functionality as much as possible and not break anything, but make it more intuitive and easier to use. This will be a very sophisticated supabase database that drives a sophisticated file processing system.





the processing for this will be in python we will have a dedicated folder for the pyton processing to handoff the fujnctionalithy to.  

However this transcribe page will follow in the footstetps of the 2 other dashboards you just did for classify and sync and help us move along the various steps required for the transcription pipeline.  Try not to break existing functionality on this transcribe page.




I need you to improve the entire sync page layout of buttons and functionality - it is too fragmented. It all starts here. 

Let's review - we will have multiple google folders we will be syncing with - 5 I can think of so far and maybe another 15 coming down the pipe. So we will need a way to identify which highe level folder we will be syncing - I know each high level folder has a unique id and that's good but we're going to need to refer to it by name.

I think we need a dashboard similar to the classify page that you just made a dashboard for.  That is an awesome dashboard but here is where it all starts 

1) new folder - recurivesly identify all its subfolders and files and create matching sync files to keep them in sync in the future.  The syncing function heavily uses sources_google but also a cluster of other tables which are used to keep statistics.  we also need the google metadata to be recorded.

2) existing folder - once the sync records are created we have to keep them in sync - identify any new ones, mark the ones no longer available with a soft delet

3) and then we need the summaries - we need to show the sync history - to be able to see where we stand from previous syncs and after syncing again we need to ujpdate that status - you have code and tables for that already

4) sonmething that will tell us about our token status, currently we get one hour based google tokens to do our dev work, the timer shoujld show us when that will expire and evne provide an optional way to refresh our token - because if we try to sync and haven't got the token we'll just fail - in fact we should not proceeed with any sync function that redquires access to the google drvie unless we have an unexpired token

5) batching of all this syncing and copyikng and audio processing and audio extraction and eventually transcrioption - will all be part of this and should be considered from the get go

6) all of this should be thought through carefully and presented in a logical order in the dashboard because I will be coming to it regularly to check on syncikng and to process any new material -

Given that synching with multiple folders is the foundation of our efforts this sync dashboard is really important.  All of these materials processed and generated will be later presented in a special presentation layer which we will design once we have all the elements

5) Just a refresher on what we do with syncing
a) it allows us to access the content of files, for docx files we use mammoth and for txt files I think you read them direclty.   
b) however we have various strategies once we get the content extracted and it varies by mime-type
c) once we have fouind new files that aren't in sources_google we have to make new records for them
d) we then need to apply our ai classification prompt that figures out which document type they are and updates the sources_google record with that informatoin.  Once it is done we don't need to redo it, and to start with it is jusrt for docx files and txt files, but eventually we will do it for pdfs
e) for m4a files that already exist on the google drive - we need to copy them locally on our dev machine from the google drive - then we will process them to get ai summaries of the videos they hold the audio for - this is the audio processing we'll be doing in our python code 
f) for mp4 files that don't have an associated m4a file we'll need to copy them temporarily (just a few at a time) and temporarily store them locally.  Then we will extract the m4a file from them so we can make the ai assisstend summaries from the videos.  I don't know whether it makes sense to copy these m4a files to the google drive after we created them locally as it would save having to do the ffmpeg extraction from the mp4 file in the future once it is done.  Also, long term storage of mp4 files in particular - but even m4a files might not be advisable - wherease we can always just read them from teh google drive  
g) all of this will use batching for the processing - and there are processing tables you have ready to use for this, and we have created all the enums and the ui will need to support this
h) just to know that after we get all the m4a files procedssed, and the ai summaries created we will return to the mp4s and create full blown transcrioptions which will require high quality audio and diaraization and intensive gpu and cpu processing if possible (and of course done proably in python).  Also there are speaker files which will be needed to sync the speakers to the transcripot so we can then process them with sophisticated ai to extract out content to go along with the video presentations - by the sync dashboard I don't think should do this last transcerioptoin processing - that will be a separte page in the app deciated to creating those cleaned and ai processed transcriptions. So you deon't need to do anything on that on the sync page for now


Let me know what else you need to know to imropve the layout and design to make it more intutive and follow the steps required.  When you redo things, try veryh hard to not break functionality if you can.







there are certain functions I need in cclassifying - here they are.  1) from the newly synced files - apply the ai to them that classifies them if they aren't calssified - right now it is just for docx files and txt files - but more will be later.   THen for any that are prsentation documents I need to run sojmething that will extract the content out - I think it is mammoth in the case of docx files and perhaps just reading the txt file from the google drive for text files, but in both cases I need to put the content into  expert_documents records and then run the ai that is specific for extracting expert json info from presentation announement documents only - and write them out to the processed_documents field if they are not alrady there, and skip if they are.

Also I need to be able to add new  document types as needed and have them show up as soon as I add them.  I like seeing all the document types at the bottom , but they could use pills based on their cateogry field so I could filter to just those that I wanted to see at a given tome.  

Some of these new document types will be json based to extract the less sturctured json into the processed_contents field and I will be needing additional document types such as these as new prompts are  created that are specific to certain document types.

I need a status of processed versus unprocssed documents from the sources_google table, but then I also need a status on the ai procssed fields that have expert_documents that have been ai processed and their new document type associated.  I guess I need this classify page to be a dashboard that helps me manage all my document classification needs - and I keep discovering them

I need a better way to organize this functionality as I will always be classifying files once I get them synced from the google drive and then subsets will be further processed by ai according to their document type

Let me know what else you need to know to imropve the layout and design to make it more intutive and follow the steps required - you have the screen shot of all the buttons I've created so far, but it is quite messy now.  When you redo things, try veryh hard to not break functionality if you can.